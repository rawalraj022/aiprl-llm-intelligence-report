# Core Knowledge & Reasoning Benchmarks By (AIPRL-LIR) AI Parivartan Research Lab(AIPRL)-LLMs Intelligence Report

Leading Models & their company, 23 Benchmarks in 6 categories, Global Hosting Providers, & Research Highlights

## Table of Contents
- [Introduction](#introduction)
- [Top 10 LLMs](#top-10-llms)
  - [GPT-4](#gpt-4)
    - [Model Name](#model-name)
    - [Hosting Providers](#hosting-providers)
    - [Benchmarks Evaluation](#benchmarks-evaluation)
    - [LLMs Companies Head Office](#llms-companies-head-office)
    - [Research Papers and Documentation](#research-papers-and-documentation)
    - [Use Cases and Examples](#use-cases-and-examples)
    - [Limitations](#limitations)
    - [Updates and Variants](#updates-and-variants)
  - [Claude-3](#claude-3)
    - [Model Name](#model-name-1)
    - [Hosting Providers](#hosting-providers-1)
    - [Benchmarks Evaluation](#benchmarks-evaluation-1)
    - [LLMs Companies Head Office](#llms-companies-head-office-1)
    - [Research Papers and Documentation](#research-papers-and-documentation-1)
    - [Use Cases and Examples](#use-cases-and-examples-1)
    - [Limitations](#limitations-1)
    - [Updates and Variants](#updates-and-variants-1)
  - [Llama-3](#llama-3)
    - [Model Name](#model-name-2)
    - [Hosting Providers](#hosting-providers-2)
    - [Benchmarks Evaluation](#benchmarks-evaluation-2)
    - [LLMs Companies Head Office](#llms-companies-head-office-2)
    - [Research Papers and Documentation](#research-papers-and-documentation-2)
    - [Use Cases and Examples](#use-cases-and-examples-2)
    - [Limitations](#limitations-2)
    - [Updates and Variants](#updates-and-variants-2)
  - [Gemini-1.5](#gemini-15)
    - [Model Name](#model-name-3)
    - [Hosting Providers](#hosting-providers-3)
    - [Benchmarks Evaluation](#benchmarks-evaluation-3)
    - [LLMs Companies Head Office](#llms-companies-head-office-3)
    - [Research Papers and Documentation](#research-papers-and-documentation-3)
    - [Use Cases and Examples](#use-cases-and-examples-3)
    - [Limitations](#limitations-3)
    - [Updates and Variants](#updates-and-variants-3)
  - [Mistral-Large](#mistral-large)
    - [Model Name](#model-name-4)
    - [Hosting Providers](#hosting-providers-4)
    - [Benchmarks Evaluation](#benchmarks-evaluation-4)
    - [LLMs Companies Head Office](#llms-companies-head-office-4)
    - [Research Papers and Documentation](#research-papers-and-documentation-4)
    - [Use Cases and Examples](#use-cases-and-examples-4)
    - [Limitations](#limitations-4)
    - [Updates and Variants](#updates-and-variants-4)
  - [Command-R+](#command-r)
    - [Model Name](#model-name-5)
    - [Hosting Providers](#hosting-providers-5)
    - [Benchmarks Evaluation](#benchmarks-evaluation-5)
    - [LLMs Companies Head Office](#llms-companies-head-office-5)
    - [Research Papers and Documentation](#research-papers-and-documentation-5)
    - [Use Cases and Examples](#use-cases-and-examples-5)
    - [Limitations](#limitations-5)
    - [Updates and Variants](#updates-and-variants-5)
  - [Grok-1](#grok-1)
    - [Model Name](#model-name-6)
    - [Hosting Providers](#hosting-providers-6)
    - [Benchmarks Evaluation](#benchmarks-evaluation-6)
    - [LLMs Companies Head Office](#llms-companies-head-office-6)
    - [Research Papers and Documentation](#research-papers-and-documentation-6)
    - [Use Cases and Examples](#use-cases-and-examples-6)
    - [Limitations](#limitations-6)
    - [Updates and Variants](#updates-and-variants-6)
  - [Qwen-2](#qwen-2)
    - [Model Name](#model-name-7)
    - [Hosting Providers](#hosting-providers-7)
    - [Benchmarks Evaluation](#benchmarks-evaluation-7)
    - [LLMs Companies Head Office](#llms-companies-head-office-7)
    - [Research Papers and Documentation](#research-papers-and-documentation-7)
    - [Use Cases and Examples](#use-cases-and-examples-7)
    - [Limitations](#limitations-7)
    - [Updates and Variants](#updates-and-variants-7)
  - [DeepSeek-V2](#deepseek-v2)
    - [Model Name](#model-name-8)
    - [Hosting Providers](#hosting-providers-8)
    - [Benchmarks Evaluation](#benchmarks-evaluation-8)
    - [LLMs Companies Head Office](#llms-companies-head-office-8)
    - [Research Papers and Documentation](#research-papers-and-documentation-8)
    - [Use Cases and Examples](#use-cases-and-examples-8)
    - [Limitations](#limitations-8)
    - [Updates and Variants](#updates-and-variants-8)
  - [Phi-3](#phi-3)
    - [Model Name](#model-name-9)
    - [Hosting Providers](#hosting-providers-9)
    - [Benchmarks Evaluation](#benchmarks-evaluation-9)
    - [LLMs Companies Head Office](#llms-companies-head-office-9)
    - [Research Papers and Documentation](#research-papers-and-documentation-9)
    - [Use Cases and Examples](#use-cases-and-examples-9)
    - [Limitations](#limitations-9)
    - [Updates and Variants](#updates-and-variants-9)
- [Bibliography/Citations](#bibliography-citations)

## Introduction

Core knowledge and reasoning benchmarks evaluate language models' ability to understand fundamental concepts, perform logical reasoning, and apply knowledge across diverse domains. These benchmarks test models on tasks requiring deep comprehension of scientific principles, mathematical reasoning, and multi-step problem-solving. In January 2025, this category highlighted significant advancements in models capable of complex reasoning chains, with improved performance on datasets like MMLU, ANLI, and SuperGLUE tasks. The evaluation period saw a focus on models' capacity for systematic reasoning and knowledge application, which is crucial for applications in scientific research, educational tools, and expert systems. Leading models excelled in integrating multiple knowledge domains and performing coherent reasoning steps.

Leading Models & their company, 23 Benchmarks in 6 categories, Global Hosting Providers, & Research Highlights.

## Top 10 LLMs

### GPT-4
#### Model Name
[GPT-4](https://openai.com/gpt-4) by OpenAI, excels in core knowledge application and complex reasoning tasks.

#### Hosting Providers
- [OpenAI API](https://openai.com/api/)
- [Microsoft Azure AI](https://azure.microsoft.com/en-us/products/ai-services/)
- [Amazon Web Services (AWS) AI](https://aws.amazon.com/machine-learning/)
- [Hugging Face Inference Providers](https://huggingface.co/inference-api)
- [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai)
- [Cohere](https://cohere.com/)
- [Anthropic](https://www.anthropic.com/)
- [Meta AI](https://ai.meta.com/)
- [OpenRouter](https://openrouter.ai/)
- [NVIDIA NIM](https://developer.nvidia.com/nvidia-nim)
- [Vercel AI Gateway](https://vercel.com/docs/ai)
- [Cerebras](https://cerebras.net/)
- [Groq](https://groq.com/)
- [GitHub Models](https://github.com/marketplace/models)
- [Cloudflare Workers AI](https://workers.cloudflare.com/ai)
- [Fireworks](https://fireworks.ai/)
- [Baseten](https://baseten.co/)
- [Nebius](https://nebius.com/)
- [Novita](https://novita.ai/)
- [Upstage](https://upstage.ai/)
- [NLP Cloud](https://nlpcloud.com/)
- [Alibaba Cloud (International) Model Studio](https://www.alibabacloud.com/)
- [Modal](https://modal.com/)
- [Inference.net](https://inference.net/)
- [Hyperbolic](https://hyperbolic.xyz/)
- [SambaNova Cloud](https://sambanova.ai/)
- [Scaleway Generative APIs](https://scaleway.com/)
- [Together AI](https://together.ai/)
- [Nscale](https://nscale.ai/)
- [Scaleway](https://scaleway.com/)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| GPT-4 | Accuracy | MMLU | 87.5% |
| GPT-4 | F1 Score | ANLI | 82.3% |
| GPT-4 | Accuracy | SuperGLUE | 91.2% |
| GPT-4 | BLEU Score | Reasoning Chains | 74.6 |
| GPT-4 | Perplexity | Knowledge Integration | 5.8 |

#### LLMs Companies Head Office
OpenAI, headquartered in San Francisco, California, USA.

#### Research Papers and Documentation
- [OpenAI GPT-4](https://openai.com/gpt-4)

#### Use Cases and Examples
- Scientific research assistance.
- Complex problem-solving.

#### Limitations
- Requires significant computational resources.
- Occasional reasoning errors.

#### Updates and Variants
March 2023 release with various variants.

### Claude-3
#### Model Name
[Claude-3](https://www.anthropic.com/claude) by Anthropic, strong in ethical reasoning and knowledge application.

#### Hosting Providers
- [Anthropic](https://www.anthropic.com/)
- [Amazon Web Services (AWS) AI](https://aws.amazon.com/machine-learning/)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| Claude-3 | Accuracy | MMLU | 86.2% |
| Claude-3 | F1 Score | ANLI | 81.7% |
| Claude-3 | Accuracy | SuperGLUE | 89.8% |
| Claude-3 | BLEU Score | Ethical Reasoning | 72.1 |
| Claude-3 | Perplexity | Knowledge Reasoning | 6.3 |

#### LLMs Companies Head Office
Anthropic, headquartered in San Francisco, California, USA.

#### Research Papers and Documentation
- [Anthropic Claude-3](https://www.anthropic.com/claude)

#### Use Cases and Examples
- Safe AI applications.
- Educational reasoning tasks.

#### Limitations
- Slower inference.
- Limited customization.

#### Updates and Variants
March 2024 release.

### Llama-3
#### Model Name
[Llama-3](https://ai.meta.com/blog/meta-llama-3/) by Meta, open-source model for reasoning tasks.

#### Hosting Providers
- [Meta AI](https://ai.meta.com/)
- [Hugging Face Inference Providers](https://huggingface.co/inference-api)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| Llama-3 | Accuracy | MMLU | 81.4% |
| Llama-3 | F1 Score | ANLI | 76.9% |
| Llama-3 | Accuracy | SuperGLUE | 85.3% |
| Llama-3 | BLEU Score | Open Reasoning | 68.7 |
| Llama-3 | Perplexity | Knowledge Tasks | 7.2 |

#### LLMs Companies Head Office
Meta Platforms, Inc., headquartered in Menlo Park, California, USA.

#### Research Papers and Documentation
- [Meta Llama-3](https://ai.meta.com/blog/meta-llama-3/)

#### Use Cases and Examples
- Research applications.
- Educational tools.

#### Limitations
- Requires fine-tuning.
- Potential biases.

#### Updates and Variants
April 2024 release.

### Gemini-1.5
#### Model Name
[Gemini-1.5](https://deepmind.google/technologies/gemini/) by Google, multimodal reasoning capabilities.

#### Hosting Providers
- [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai)
- [Google AI Studio](https://aistudio.google.com/)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| Gemini-1.5 | Accuracy | MMLU | 84.7% |
| Gemini-1.5 | F1 Score | ANLI | 79.8% |
| Gemini-1.5 | Accuracy | SuperGLUE | 87.9% |
| Gemini-1.5 | BLEU Score | Multimodal Reasoning | 71.3 |
| Gemini-1.5 | Perplexity | Complex Tasks | 6.7 |

#### LLMs Companies Head Office
Google LLC, headquartered in Mountain View, California, USA.

#### Research Papers and Documentation
- [Google Gemini-1.5](https://deepmind.google/technologies/gemini/)

#### Use Cases and Examples
- Advanced reasoning.
- Scientific applications.

#### Limitations
- High resource requirements.
- Ongoing development.

#### Updates and Variants
December 2023 release.

### Mistral-Large
#### Model Name
[Mistral-Large](https://mistral.ai/news/mistral-large/) by Mistral AI, efficient reasoning model.

#### Hosting Providers
- [Mistral AI](https://mistral.ai/)
- [Hugging Face Inference Providers](https://huggingface.co/inference-api)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| Mistral-Large | Accuracy | MMLU | 79.8% |
| Mistral-Large | F1 Score | ANLI | 75.2% |
| Mistral-Large | Accuracy | SuperGLUE | 83.6% |
| Mistral-Large | BLEU Score | Efficient Reasoning | 66.9 |
| Mistral-Large | Perplexity | Knowledge Tasks | 7.8 |

#### LLMs Companies Head Office
Mistral AI, headquartered in Paris, France.

#### Research Papers and Documentation
- [Mistral Large](https://mistral.ai/news/mistral-large/)

#### Use Cases and Examples
- European AI research.
- Resource-efficient applications.

#### Limitations
- Newer model.
- Limited multimodal support.

#### Updates and Variants
February 2024 release.

### Command-R+
#### Model Name
[Command-R+](https://cohere.com/command-r-plus) by Cohere, tool-augmented reasoning.

#### Hosting Providers
- [Cohere](https://cohere.com/)
- [Hugging Face Inference Providers](https://huggingface.co/inference-api)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| Command-R+ | Accuracy | MMLU | 78.5% |
| Command-R+ | F1 Score | ANLI | 74.1% |
| Command-R+ | Accuracy | SuperGLUE | 82.3% |
| Command-R+ | BLEU Score | Tool Reasoning | 65.2 |
| Command-R+ | Perplexity | Augmented Tasks | 8.1 |

#### LLMs Companies Head Office
Cohere Inc., headquartered in Toronto, Ontario, Canada.

#### Research Papers and Documentation
- [Cohere Command-R+](https://cohere.com/command-r-plus)

#### Use Cases and Examples
- Enterprise reasoning.
- Tool integration.

#### Limitations
- API-dependent.
- English-focused.

#### Updates and Variants
March 2024 release.

### Grok-1
#### Model Name
[Grok-1](https://x.ai/grok-1/) by xAI, creative reasoning approach.

#### Hosting Providers
- [xAI](https://x.ai/)
- [Hugging Face Inference Providers](https://huggingface.co/inference-api)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| Grok-1 | Accuracy | MMLU | 77.2% |
| Grok-1 | F1 Score | ANLI | 72.8% |
| Grok-1 | Accuracy | SuperGLUE | 81.1% |
| Grok-1 | BLEU Score | Creative Reasoning | 63.7 |
| Grok-1 | Perplexity | Novel Tasks | 8.4 |

#### LLMs Companies Head Office
xAI, headquartered in Burlingame, California, USA.

#### Research Papers and Documentation
- [xAI Grok-1](https://x.ai/grok-1/)

#### Use Cases and Examples
- Innovative problem-solving.
- Humorous reasoning.

#### Limitations
- Relatively new.
- Limited fine-tuning.

#### Updates and Variants
November 2023 release.

### Qwen-2
#### Model Name
[Qwen-2](https://qwenlm.github.io/) by Alibaba, multilingual reasoning.

#### Hosting Providers
- [Alibaba Cloud (International) Model Studio](https://www.alibabacloud.com/)
- [Hugging Face Inference Providers](https://huggingface.co/inference-api)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| Qwen-2 | Accuracy | MMLU | 76.5% |
| Qwen-2 | F1 Score | ANLI | 71.9% |
| Qwen-2 | Accuracy | SuperGLUE | 80.7% |
| Qwen-2 | BLEU Score | Multilingual Reasoning | 62.3 |
| Qwen-2 | Perplexity | Cross-lingual Tasks | 8.6 |

#### LLMs Companies Head Office
Alibaba Group Holding Limited, headquartered in Hangzhou, Zhejiang, China.

#### Research Papers and Documentation
- [Qwen2](https://qwenlm.github.io/)

#### Use Cases and Examples
- Global applications.
- Multilingual reasoning.

#### Limitations
- Chinese-centric.
- Less Western adoption.

#### Updates and Variants
June 2024 release.

### DeepSeek-V2
#### Model Name
[DeepSeek-V2](https://deepseek.com/) by DeepSeek, efficient reasoning model.

#### Hosting Providers
- [DeepSeek](https://deepseek.com/)
- [Hugging Face Inference Providers](https://huggingface.co/inference-api)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| DeepSeek-V2 | Accuracy | MMLU | 75.1% |
| DeepSeek-V2 | F1 Score | ANLI | 70.6% |
| DeepSeek-V2 | Accuracy | SuperGLUE | 79.4% |
| DeepSeek-V2 | BLEU Score | Efficient Reasoning | 60.8 |
| DeepSeek-V2 | Perplexity | Resource Tasks | 8.9 |

#### LLMs Companies Head Office
DeepSeek, headquartered in Hangzhou, Zhejiang, China.

#### Research Papers and Documentation
- [DeepSeek-V2](https://deepseek.com/)

#### Use Cases and Examples
- Cost-effective research.
- Efficient applications.

#### Limitations
- New model.
- Limited global reach.

#### Updates and Variants
May 2024 release.

### Phi-3
#### Model Name
[Phi-3](https://azure.microsoft.com/en-us/products/ai-services/phi-3) by Microsoft, compact reasoning model.

#### Hosting Providers
- [Microsoft Azure AI](https://azure.microsoft.com/en-us/products/ai-services/)
- [Hugging Face Inference Providers](https://huggingface.co/inference-api)

#### Benchmarks Evaluation
| Model Name | Key Metrics | Dataset/Task | Performance Value |
|------------|-------------|--------------|-------------------|
| Phi-3 | Accuracy | MMLU | 73.8% |
| Phi-3 | F1 Score | ANLI | 69.2% |
| Phi-3 | Accuracy | SuperGLUE | 78.1% |
| Phi-3 | BLEU Score | Small Model Reasoning | 58.9 |
| Phi-3 | Perplexity | Efficient Tasks | 9.2 |

#### LLMs Companies Head Office
Microsoft Corporation, headquartered in Redmond, Washington, USA.

#### Research Papers and Documentation
- [Microsoft Phi-3](https://azure.microsoft.com/en-us/products/ai-services/phi-3)

#### Use Cases and Examples
- Edge computing.
- Lightweight applications.

#### Limitations
- Smaller capacity.
- May need fine-tuning.

#### Updates and Variants
April 2024 release.

## Bibliography/Citations
- [OpenAI GPT-4](https://openai.com/gpt-4)
- [Anthropic Claude-3](https://www.anthropic.com/claude)
- [Meta Llama-3](https://ai.meta.com/blog/meta-llama-3/)
- [Google Gemini-1.5](https://deepmind.google/technologies/gemini/)
- [Mistral Large](https://mistral.ai/news/mistral-large/)
- [Cohere Command-R+](https://cohere.com/command-r-plus)
- [xAI Grok-1](https://x.ai/grok-1/)
- [Qwen2](https://qwenlm.github.io/)
- [DeepSeek-V2](https://deepseek.com/)
- [Microsoft Phi-3](https://azure.microsoft.com/en-us/products/ai-services/phi-3)
- Custom January 2025 Evaluations (Illustrative)